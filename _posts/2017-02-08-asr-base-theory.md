---
layout: post
title: 深度学习与语音识别：语音识别的基本原理
date: 2017-02-08 18:35:30 +08:00
category: "ASR"
tags: [ASR]
---


在阅读《解析深度学习：语音识别实践》时，也会参考一些博客，看到好的也会整理一下分享出来。本文主要通过浅显的语言来展示语音识别的基本过程和原理。本文大部分内容转载至：[语音识别的技术原理是什么？](https://www.zhihu.com/question/20398418)。

语音识别技术，也被称为自动语音识别AutomaTIc Speech RecogniTIon（ASR），其目标是将人类的语音中的词汇内容转换为计算机可读的输入，例如按键、二进制编码或者字符序列。与说话人识别及说话人确认不同，后者尝试识别或确认发出语音的说话人而非其中所包含的词汇内容。下面我们来详细解读语音识别技术原理。

首先，我们知道声音实际上是一种波。常见的mp3等格式都是压缩格式，必须转成非压缩的纯波形文件来处理，比如Windows PCM文件，也就是俗称的wav文件。wav文件里存储的除了一个文件头以外，就是声音波形的一个个点了。下图是一个波形的示例。

![](https://pic2.zhimg.com/80/c58a25274bcc8788831e6100e9e366fd_hd.jpg)

在开始语音识别之前，有时需要把首尾端的静音切除，降低对后续步骤造成的干扰。这个静音切除的操作一般称为VAD，需要用到信号处理的一些技术。要对声音进行分析，需要对声音分帧，也就是把声音切开成一小段一小段，每小段称为一帧。分帧操作一般不是简单的切开，而是使用移动窗函数来实现，这里不详述。帧与帧之间一般是有交叠的，就像下图这样：

![](https://pic7.zhimg.com/80/554db0c9107727aa1e31a62cca782ada_hd.jpg)

图中，每帧的长度为25毫秒，每两帧之间有25-10=15毫秒的交叠。我们称为以帧长25ms、帧移10ms分帧。分帧后，语音就变成了很多小段。但波形在时域上几乎没有描述能力，因此必须将波形作变换。常见的一种变换方法是提取MFCC特征，根据人耳的生理特性，把每一帧波形变成一个多维向量，可以简单地理解为这个向量包含了这帧语音的内容信息。这个过程叫做声学特征提取。实际应用中，这一步有很多细节，声学特征也不止有MFCC这一种，具体这里不讲。

至此，声音就成了一个12行（假设声学特征是12维）、N列的一个矩阵，称之为观察序列，这里N为总帧数。观察序列如下图所示，图中，每一帧都用一个12维的向量表示，色块的颜色深浅表示向量值的大小。

![](https://pic4.zhimg.com/80/c5cc0131dc6d38e871953eea0792f7db_hd.jpg)

接下来就要介绍怎样把这个矩阵变成文本了。首先要介绍两个概念：

* 音素：单词的发音由音素构成。对英语，一种常用的音素集是卡内基梅隆大学的一套由39个音素构成的音素集‎。汉语一般直接用全部声母和韵母作为音素集，另外汉语识别还分有调无调，不详述。
* 状态：这里理解成比音素更细致的语音单位就行啦。通常把一个音素划分成3个状态。

语音识别是怎么工作的呢？实际上一点都不神秘，无非是：

1. 第一步，把帧识别成状态（难点）。
2. 第二步，把状态组合成音素。
3. 第三步，把音素组合成单词。如下图所示：

![](https://pic1.zhimg.com/80/beee2a3ce7b1b56de362c9015e5b8ccd_hd.jpg)

图中，每个小竖条代表一帧，若干帧语音对应一个状态，每三个状态组合成一个音素，若干个音素组合成一个单词。也就是说，只要知道每帧语音对应哪个状态了，语音识别的结果也就出来了。那每帧音素对应哪个状态呢？有个容易想到的办法，看某帧对应哪个状态的概率最大，那这帧就属于哪个状态。比如下面的示意图，这帧在状态S3上的条件概率最大，因此就猜这帧属于状态S3。


![](https://pic3.zhimg.com/80/61d5bff9c239e7519e92d98f5bc44689_hd.jpg)

那这些用到的概率从哪里读取呢？有个叫“声学模型”的东西，里面存了一大堆参数，通过这些参数，就可以知道帧和状态对应的概率。获取这一大堆参数的方法叫做“训练”，需要使用巨大数量的语音数据，训练的方法比较繁琐，这里不讲。

但这样做有一个问题：每一帧都会得到一个状态号，最后整个语音就会得到一堆乱七八糟的状态号，相邻两帧间的状态号基本都不相同。假设语音有1000帧，每帧对应1个状态，每3个状态组合成一个音素，那么大概会组合成300个音素，但这段语音其实根本没有这么多音素。如果真这么做，得到的状态号可能根本无法组合成音素。实际上，相邻帧的状态应该大多数都是相同的才合理，因为每帧很短。

解决这个问题的常用方法就是使用隐马尔可夫模型（Hidden Markov Model，HMM）。这东西听起来好像很高深的样子，实际上用起来很简单：

1. 第一步，构建一个状态网络。
2. 第二步，从状态网络中寻找与声音最匹配的路径。

这样就把结果限制在预先设定的网络中，避免了刚才说到的问题，当然也带来一个局限，比如你设定的网络里只包含了“今天晴天”和“今天下雨”两个句子的状态路径，那么不管说些什么，识别出的结果必然是这两个句子中的一句。

那如果想识别任意文本呢？把这个网络搭得足够大，包含任意文本的路径就可以了。但这个网络越大，想要达到比较好的识别准确率就越难。所以要根据实际任务的需求，合理选择网络大小和结构。

搭建状态网络，是由单词级网络展开成音素网络，再展开成状态网络。语音识别过程其实就是在状态网络中搜索一条最佳路径，语音对应这条路径的概率最大，这称之为“解码”。路径搜索的算法是一种动态规划剪枝的算法，称之为Viterbi算法，用于寻找全局最优路径。

![](https://pic2.zhimg.com/80/91e4cb3b716b63d5e18ba710a1a2d770_hd.jpg)

这里所说的累积概率，由三部分构成，分别是：

* 观察概率：每帧和每个状态对应的概率
* 转移概率：每个状态转移到自身或转移到下个状态的概率
* 语言概率：根据语言统计规律得到的概率

其中，前两种概率从声学模型中获取，最后一种概率从语言模型中获取。语言模型是使用大量的文本训练出来的，可以利用某门语言本身的统计规律来帮助提升识别正确率。语言模型很重要，如果不使用语言模型，当状态网络较大时，识别出的结果基本是一团乱麻。

这样基本上语音识别过程就完成了。